{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.utils\n",
    "import torchvision.transforms\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageDraw, Image\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PYTORCH_CUDA_ALLOC_CONF={'max_split_size_mb':'1024','garbage_collection_threshold':'0.6'}\n",
    "torch.set_printoptions(threshold=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_dir = \"../Cyclist_Detection/Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing our Dataset class\n",
    "class YoloPtDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset with tensor format images and Yolo format labels\"\"\"\n",
    "\n",
    "    def __init__(self,root_dir,data_type=None,transform=None):\n",
    "        if data_type is None:\n",
    "            try:\n",
    "                self.paths = pd.read_csv(os.path.join(root_dir,\"src\",\"src.csv\"))\n",
    "                self.paths.values.astype(str)\n",
    "            except:\n",
    "                raise TypeError(\"The directory doesn't have a src file with the paths to all the images\")\n",
    "        elif data_type == 'train' or data_type == 'Train':\n",
    "            try:\n",
    "                self.paths = pd.read_csv(os.path.join(root_dir,\"src\",\"train_src.csv\"))\n",
    "                self.paths.values.astype(str)\n",
    "            except:\n",
    "                raise TypeError(\"The directory doesn't have a train_src file with the paths to all the images\")\n",
    "\n",
    "        elif data_type == 'val' or data_type == 'Validate' or data_type == 'validate':\n",
    "            try:\n",
    "                self.paths = pd.read_csv(os.path.join(root_dir,\"src\",\"val_src.csv\"))\n",
    "                self.paths.values.astype(str)\n",
    "            except:\n",
    "                raise TypeError(\"The directory doesn't have a val_src file with the paths to all the images\")\n",
    "\n",
    "        elif data_type == 'test' or data_type == 'Test':\n",
    "            try:\n",
    "                self.paths = pd.read_csv(os.path.join(root_dir,\"src\",\"test_src.csv\"))\n",
    "                self.paths.values.astype(str)\n",
    "            except:\n",
    "                raise TypeError(\"The directory doesn't have a test_src file with the paths to all the images\")\n",
    "        else:\n",
    "            raise ValueError(\"Wrong argument has been passed through data_type, must be one of: train,test,val\")\n",
    "\n",
    "\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        #To do implement transformation\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path, label_path = self.paths.iloc[idx,1:].values\n",
    "\n",
    "        return torch.load(img_path),torch.load(label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Size transformation for Dataloader\n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "        matched to output_size. If int, smaller of image edges is matched\n",
    "        to output_size keeping aspect ratio the same.\n",
    "\"\"\"\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img,label = sample[0],sample[1]\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = torchvision.transforms.resize()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Modyifying yolo formatted coordinates to (x_left,y_left,x_right,y_right)\n",
    "def yolo_to_xy(yolo):\n",
    "    _yolo = yolo.clone()\n",
    "    w,h = yolo[:,2]/2,yolo[:,3]/2\n",
    "    _yolo[:,0] = yolo[:,0]-w\n",
    "    _yolo[:,1] = yolo[:,1]-h\n",
    "    _yolo[:,2] = yolo[:,0]+w\n",
    "    _yolo[:,3] = yolo[:,1]+h\n",
    "    return _yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def xy_to_yolo(xy):\n",
    "    if xy.shape[1] == 5:\n",
    "        x1,y1,x2,y2 = xy[:,1],xy[:,2],xy[:,3],xy[:,4]\n",
    "        cx = (x1 +x2)/2\n",
    "        cy = (y1 + y2)/2\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1\n",
    "\n",
    "        return torch.stack((xy[:,0],cx,cy,w,h),axis=-1)\n",
    "\n",
    "    elif xy.shape[1] == 4:\n",
    "        x1,y1,x2,y2 = xy[:,0],xy[:,1],xy[:,2],xy[:,3]\n",
    "        cx = (x1 +x2)/2\n",
    "        cy = (y1 + y2)/2\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1\n",
    "\n",
    "        return torch.stack((cx,cy,w,h),axis=-1)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f'Wrong input shape:{xy.shape},takes [i,4] or [i,5]')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Displaying a single image with bounding boxes\n",
    "def Display_yolo(data,colour='green',width = 5,dtype='yolo'):\n",
    "\n",
    "    img, label = data[0],data[1]\n",
    "    resolution = img.size()[1:3]\n",
    "    trans = torchvision.transforms.ToPILImage()\n",
    "    img = trans(img)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    #Removing negative padding\n",
    "    for i in range(len(label)):\n",
    "        if label[i][0] == -1:\n",
    "            label = label[0:i]\n",
    "            break\n",
    "\n",
    "    #Transforming to bounding box xy coordinates\n",
    "    if dtype == 'yolo':\n",
    "        label = yolo_to_xy(label[:,1:5])\n",
    "    else:\n",
    "        label = label[:,1:5]\n",
    "\n",
    "\n",
    "    label[:,0],label[:,2] = label[:,0]*resolution[1],label[:,2]*resolution[1]\n",
    "    label[:,1],label[:,3] = label[:,1]*resolution[0],label[:,3]*resolution[0]\n",
    "    label = label.tolist()\n",
    "\n",
    "    #Re-calculates the yolo format to top-left and bottom right points\n",
    "    for i in label:\n",
    "        draw.rectangle(i,outline=colour,width=width)\n",
    "\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = YoloPtDataset(Dataset_dir,data_type='train')\n",
    "val = YoloPtDataset(Dataset_dir,data_type='val')\n",
    "test = YoloPtDataset(Dataset_dir,data_type='test')\n",
    "train_iter = torch.utils.data.DataLoader(train,batch_size=2,shuffle=True)\n",
    "val_iter = torch.utils.data.DataLoader(val,batch_size=1,shuffle=False)\n",
    "test_iter = torch.utils.data.DataLoader(test,batch_size=len(test),shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Display_yolo(train[402],colour='yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Works correctly\n",
    "#Add handling for label type \"xy\"\n",
    "def Intersection_Over_Union(anchors,bounding_boxes,ltype='yolo'):\n",
    "    if ltype == 'yolo':\n",
    "        if bounding_boxes.shape[1] == 5:\n",
    "            b1 = yolo_to_xy(anchors)\n",
    "            b2 = yolo_to_xy(bounding_boxes[:,1:5])\n",
    "            bounding_areas = bounding_boxes[:,3]*bounding_boxes[:,4]\n",
    "        elif bounding_boxes.shape[1] == 4:\n",
    "            b1 = yolo_to_xy(anchors)\n",
    "            b2 = yolo_to_xy(bounding_boxes)\n",
    "            bounding_areas = bounding_boxes[:,2]*bounding_boxes[:,3]\n",
    "        else:\n",
    "            raise ValueError(f'Wrong shape of input {bounding_boxes.shape}, takes input [i,5] or [i,4]')\n",
    "\n",
    "    else:\n",
    "        b1 = anchors.clone()\n",
    "        b2 = bounding_boxes[:,1:5]\n",
    "\n",
    "\n",
    "    inter_upperlefts = torch.max(b1[:, None, :2], b2[:, :2])\n",
    "    inter_lowerrights = torch.min(b1[:, None, 2:], b2[:, 2:])\n",
    "\n",
    "\n",
    "\n",
    "    inters = (inter_lowerrights-inter_upperlefts).clamp(min=0)\n",
    "    intersection = inters[:, :, 0] * inters[:, :, 1]\n",
    "\n",
    "    anchor_areas = anchors[:,2]*anchors[:,3]\n",
    "\n",
    "\n",
    "    inter_areas = inters[:, :, 0] * inters[:, :, 1]\n",
    "    union_areas = anchor_areas[:, None] + bounding_areas - inter_areas\n",
    "\n",
    "    return inter_areas/union_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Works correctly\n",
    "def assign_anchor_to_bbox(anchors,ground_truth,device='cpu',iou_treshold = 0.7):\n",
    "    \"\"\"Assign closest ground-truth bounding boxes to anchor boxes.\"\"\"\n",
    "    num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]\n",
    "    # Element x_ij in the i-th row and j-th column is the IoU of the anchor\n",
    "    # box i and the ground-truth bounding box j\n",
    "    jaccard = Intersection_Over_Union(anchors, ground_truth)\n",
    "    # Initialize the tensor to hold the assigned ground-truth bounding box for\n",
    "    # each anchor\n",
    "    anchors_bbox_map = torch.full((num_anchors,), -1, dtype=torch.long,\n",
    "                              device=device)\n",
    "    # Assign ground-truth bounding boxes according to the threshold\n",
    "    max_ious, indices = torch.max(jaccard, dim=1)\n",
    "    anc_i = torch.nonzero(max_ious >= 0.5).reshape(-1)\n",
    "    box_j = indices[max_ious >= 0.5]\n",
    "    anchors_bbox_map[anc_i] = box_j\n",
    "    col_discard = torch.full((num_anchors,), -1)\n",
    "    row_discard = torch.full((num_gt_boxes,), -1)\n",
    "    for _ in range(num_gt_boxes):\n",
    "        max_idx = torch.argmax(jaccard)  # Find the largest IoU\n",
    "        box_idx = (max_idx % num_gt_boxes).long()\n",
    "        anc_idx = (max_idx / num_gt_boxes).long()\n",
    "        anchors_bbox_map[anc_idx] = box_idx\n",
    "        jaccard[:, box_idx] = col_discard\n",
    "        jaccard[anc_idx, :] = row_discard\n",
    "    return anchors_bbox_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculating offsets of two boxes\n",
    "#Works correctly\n",
    "def calc_offsets(anchors,bounding_boxes,eps=1e-6):\n",
    "    offset_xy = 10 * (bounding_boxes[:, :2] - anchors[:, :2]) / anchors[:, 2:]\n",
    "    offset_wh = 5 * torch.log(eps + bounding_boxes[:, 2:] / anchors[:, 2:])\n",
    "    return torch.cat([offset_xy, offset_wh], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Adjusting anchors' position based on their predicted offsets\n",
    "#Works correctly\n",
    "def offset_boxes(anchors,offset_preds):\n",
    "    \"\"\"Predict bounding boxes based on anchor boxes with predicted offsets.\"\"\"\n",
    "    pred_bbox_xy = (offset_preds[:, :2] * anchors[:, 2:] / 10) + anchors[:, :2]\n",
    "    pred_bbox_wh = torch.exp(offset_preds[:, 2:] / 5) * anchors[:, 2:]\n",
    "    return torch.cat((pred_bbox_xy, pred_bbox_wh), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Works correctly\n",
    "def multibox_target(anchors, labels,iou_threshold = 0.5):\n",
    "    \"\"\"Label anchor boxes using ground-truth bounding boxes.\"\"\"\n",
    "\n",
    "    batch_size, anchors = labels.shape[0], anchors.squeeze(0)\n",
    "    batch_offset, batch_mask, batch_class_labels = [], [], []\n",
    "    device, num_anchors = anchors.device, anchors.shape[0]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "\n",
    "        label = labels[i, :, :]\n",
    "        anchors_bbox_map = assign_anchor_to_bbox(\n",
    "            anchors,label[:,1:], device,iou_treshold=iou_threshold)\n",
    "        bbox_mask = ((anchors_bbox_map >= 0).float().unsqueeze(-1)).repeat(\n",
    "            1, 4)\n",
    "\n",
    "        # Initialize class labels and assigned bounding box coordinates with\n",
    "        # zeros\n",
    "        class_labels = torch.zeros(num_anchors, dtype=torch.long,\n",
    "                                   device=device)\n",
    "        assigned_bb = torch.zeros((num_anchors, 4), dtype=torch.float32,\n",
    "                                  device=device)\n",
    "\n",
    "        # Label classes of anchor boxes using their assigned ground-truth\n",
    "        # bounding boxes. If an anchor box is not assigned any, we label its\n",
    "        # class as background (the value remains zero)\n",
    "        indices_true = torch.nonzero(anchors_bbox_map >= 0)\n",
    "        bb_idx = anchors_bbox_map[indices_true]\n",
    "        class_labels[indices_true] = label[bb_idx, 0].long() + 1\n",
    "        assigned_bb[indices_true] = label[bb_idx, 1:]\n",
    "\n",
    "        # Offset transformation\n",
    "        offset = calc_offsets(anchors, assigned_bb) * bbox_mask\n",
    "        batch_offset.append(offset.reshape(-1))\n",
    "        batch_mask.append(bbox_mask.reshape(-1))\n",
    "        batch_class_labels.append(class_labels)\n",
    "\n",
    "    bbox_offset = torch.stack(batch_offset)\n",
    "    bbox_mask = torch.stack(batch_mask)\n",
    "    class_labels = torch.stack(batch_class_labels)\n",
    "    return (bbox_offset, bbox_mask, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ground_truth_ = torch.tensor([[0, 0.1, 0.08, 0.52, 0.92],\n",
    "                             [1, 0.55, 0.2, 0.9, 0.88]])\n",
    "anchors_ = torch.tensor([[0, 0.1, 0.2, 0.3], [0.15, 0.2, 0.4, 0.4],\n",
    "                        [0.63, 0.05, 0.88, 0.98], [0.66, 0.45, 0.8, 0.8],\n",
    "                        [0.57, 0.3, 0.92, 0.9]])\n",
    "ground_truth_ = xy_to_yolo(ground_truth_)\n",
    "anchors_ = xy_to_yolo(anchors_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Non-maximum-suppression\n",
    "#Works correctly\n",
    "def nms(boxes,scores,iou_treshold):\n",
    "    \"\"\"Sort confidence scores of predicted bounding boxes.\"\"\"\n",
    "    B = torch.argsort(scores, dim=-1, descending=True)\n",
    "    keep = []  # Indices of predicted bounding boxes that will be kept\n",
    "    while B.numel() > 0:\n",
    "        i = B[0].to('cpu')\n",
    "        keep.append(i)\n",
    "        if B.numel() == 1: break\n",
    "        iou = Intersection_Over_Union(boxes[i, :].reshape(-1, 4),\n",
    "                    boxes[B[1:], :].reshape(-1, 4)).reshape(-1)\n",
    "        inds = torch.nonzero(iou <= iou_treshold).reshape(-1)\n",
    "        B = B[inds + 1]\n",
    "    return torch.tensor(keep, device=boxes.device)\n",
    "\n",
    "def non_maximum_suppression(cls_probs, offset_preds, anchors, nms_threshold=0.5,\n",
    "                       pos_threshold=0.009999999):\n",
    "    \"\"\"Predict bounding boxes using non-maximum suppression.\"\"\"\n",
    "    device, batch_size = cls_probs.device, cls_probs.shape[0]\n",
    "    anchors = anchors.squeeze(0)\n",
    "    num_classes, num_anchors = cls_probs.shape[1], cls_probs.shape[2]\n",
    "    out = []\n",
    "    for i in range(batch_size):\n",
    "\n",
    "        cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape(-1, 4)\n",
    "        conf, class_id = torch.max(cls_prob[1:], 0)\n",
    "\n",
    "        #Uses yolo coordinates instead of xy\n",
    "        predicted_bb = offset_boxes(anchors, offset_pred)\n",
    "        keep = nms(predicted_bb, conf, nms_threshold)\n",
    "\n",
    "        # Find all non-`keep` indices and set the class to background\n",
    "        all_idx = torch.arange(num_anchors, dtype=torch.long, device=device)\n",
    "        combined = torch.cat((keep, all_idx))\n",
    "        uniques, counts = combined.unique(return_counts=True)\n",
    "        non_keep = uniques[counts == 1]\n",
    "        all_id_sorted = torch.cat((keep, non_keep))\n",
    "        class_id[non_keep] = -1\n",
    "        class_id = class_id[all_id_sorted]\n",
    "        conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]\n",
    "        # Here `pos_threshold` is a threshold for positive (non-background)\n",
    "        # predictions\n",
    "        below_min_idx = (conf < pos_threshold)\n",
    "        class_id[below_min_idx] = -1\n",
    "        conf[below_min_idx] = 1 - conf[below_min_idx]\n",
    "        pred_info = torch.cat((class_id.unsqueeze(1),\n",
    "                               conf.unsqueeze(1),\n",
    "                               predicted_bb), dim=1)\n",
    "        out.append(pred_info)\n",
    "\n",
    "        #Gives an output [1,num_anchors,6]\n",
    "        #[0,:,0] - removal indicator yes (-1) no (1), [0,:,1] - confidence, [0,:,2:6] - yolo coordinates\n",
    "    return torch.stack(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Works correctly\n",
    "def multibox_prior(data,sizes,ratios):\n",
    "    \"\"\"Generate anchor boxes with different shapes centered on each pixel.\"\"\"\n",
    "    in_height, in_width = data.shape[-2:]\n",
    "    device, num_sizes, num_ratios = data.device, len(sizes), len(ratios)\n",
    "    boxes_per_pixel = (num_sizes + num_ratios - 1)\n",
    "    size_tensor = torch.tensor(sizes, device=device)\n",
    "    ratio_tensor = torch.tensor(ratios, device=device)\n",
    "\n",
    "\n",
    "    # Generate all center points for the anchor boxes\n",
    "    # Offsets are required to move the anchor to the center of a pixel. Since\n",
    "    # a pixel has height=1 and width=1, we choose to offset our centers by 0.5\n",
    "    center_w = (torch.arange(in_width, device=device) + 0.5) / in_width\n",
    "    center_h = (torch.arange(in_height, device=device) + 0.5) / in_height\n",
    "    shift_y, shift_x = torch.meshgrid(center_h, center_w)\n",
    "    shift_y, shift_x = shift_y.reshape(-1), shift_x.reshape(-1)\n",
    "    zeros_xy = torch.zeros(len(shift_x),device=device)\n",
    "\n",
    "\n",
    "    # Generate all widths and heights for the anchor boxes\n",
    "    w = torch.cat((size_tensor * torch.sqrt(ratio_tensor[0]),\n",
    "                   sizes[0] * torch.sqrt(ratio_tensor[1:])))\\\n",
    "                   / in_width  # Handle rectangular inputs\n",
    "    h = torch.cat((size_tensor / torch.sqrt(ratio_tensor[0]),\n",
    "                   sizes[0] / torch.sqrt(ratio_tensor[1:])))\\\n",
    "                    / in_height\n",
    "    zeros_wh = torch.zeros(len(w),device=device)\n",
    "\n",
    "\n",
    "\n",
    "    anchor_manipulations = torch.stack((zeros_wh, zeros_wh, w, h)).T.repeat(\n",
    "        in_height * in_width, 1)\n",
    "\n",
    "    out_grid = torch.stack([shift_x, shift_y, zeros_xy, zeros_xy],\n",
    "                           dim=1).repeat_interleave(boxes_per_pixel, dim=0)\n",
    "\n",
    "    output = out_grid + anchor_manipulations\n",
    "\n",
    "    return output.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Functions for SSD\n",
    "def cls_predictor(num_inputs,priors,classes):\n",
    "    return nn.Conv2d(num_inputs,priors*(classes+1),kernel_size=3,padding=1)\n",
    "\n",
    "def offset_predictor(num_inputs,priors):\n",
    "    return nn.Conv2d(num_inputs,priors*4,kernel_size=3,padding=1)\n",
    "\n",
    "def flatten_pred(pred):\n",
    "    return torch.flatten(pred.permute(0,2,3,1),start_dim=1)\n",
    "\n",
    "def concat_pred(pred):\n",
    "    return torch.cat([flatten_pred(p) for p in pred], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Building blocks of SSD\n",
    "def SSD_Conv_Block(num_inputs,num_outputs,convolutions=2,downsample=2):\n",
    "    blk = []\n",
    "    for _ in range(convolutions):\n",
    "        blk.append(nn.Conv2d(num_inputs,num_outputs,kernel_size=3,padding=1))\n",
    "        blk.append(nn.BatchNorm2d(num_outputs))\n",
    "        blk.append(nn.ReLU())\n",
    "        num_inputs = num_outputs\n",
    "    blk.append(nn.MaxPool2d(downsample))\n",
    "    return nn.Sequential(*blk)\n",
    "\n",
    "\n",
    "def SSD_Base_Block(channels):\n",
    "    if not isinstance(channels,list):\n",
    "        raise TypeError(\"channels argument must be list of channels\")\n",
    "\n",
    "    blk = []\n",
    "    for i in range(len(channels)-1):\n",
    "        blk.append(SSD_Conv_Block(channels[i],channels[i+1]))\n",
    "    return nn.Sequential(*blk)\n",
    "\n",
    "def SSD_Block_Forward(data,blk,sizes,ratios,cls_predictor,offset_predictor):\n",
    "    Y = blk(data)\n",
    "    priors = multibox_prior(Y,sizes=sizes,ratios=ratios)\n",
    "    cls_preds = cls_predictor(Y)\n",
    "    offset_preds = offset_predictor(Y)\n",
    "    return Y,priors, cls_preds, offset_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SSD(nn.Module):\n",
    "    def __init__(self,classes,base_channels,remaining_channels,sizes,ratios,**kwargs):\n",
    "        super(SSD,self).__init__(**kwargs)\n",
    "\n",
    "        if not isinstance(base_channels,list):\n",
    "            raise TypeError(\"base_channels argument must be list of channels\")\n",
    "\n",
    "        if not isinstance(remaining_channels,list):\n",
    "            raise TypeError(\"remaining_channels argument must be list of channels\")\n",
    "\n",
    "        self.remaining_channels = remaining_channels\n",
    "        self.sizes = sizes\n",
    "        self.ratios = ratios\n",
    "        self.priors = len(self.sizes[0]) + len(self.ratios[0]) - 1\n",
    "\n",
    "        if not len(self.remaining_channels) == self.priors+1:\n",
    "            raise TypeError(f'The amount of priors {self.priors+1}, does not match amount of layers {len(self.remaining_channels)}')\n",
    "\n",
    "\n",
    "        self.classes = classes\n",
    "        self.base_channels = base_channels\n",
    "\n",
    "\n",
    "\n",
    "        self.blk_0 = SSD_Base_Block(self.base_channels)\n",
    "        self.cls_0 = cls_predictor(self.base_channels[-1],self.priors,self.classes)\n",
    "        self.ofs_0 = offset_predictor(self.base_channels[-1],self.priors)\n",
    "\n",
    "\n",
    "        for i in range(len(self.remaining_channels)):\n",
    "\n",
    "            if i == len(self.remaining_channels)-2:\n",
    "\n",
    "                setattr(self,f'blk_{i+1}',nn.AdaptiveMaxPool2d((1,1)))\n",
    "                setattr(self,f'cls_{i+1}',cls_predictor(self.remaining_channels[i+1],self.priors,self.classes))\n",
    "                setattr(self,f'ofs_{i+1}',offset_predictor(self.remaining_channels[i+1],self.priors))\n",
    "                self.pred_layers = i+2\n",
    "\n",
    "                break\n",
    "\n",
    "\n",
    "            setattr(self,f'blk_{i+1}',SSD_Conv_Block(self.remaining_channels[i],self.remaining_channels[i+1]))\n",
    "            setattr(self,f'cls_{i+1}',cls_predictor(self.remaining_channels[i+1],self.priors,self.classes))\n",
    "            setattr(self,f'ofs_{i+1}',offset_predictor(self.remaining_channels[i+1],self.priors))\n",
    "\n",
    "    def forward(self,X):\n",
    "        anchors, cls_preds,ofs_preds = [None]*self.pred_layers, [None]*self.pred_layers,[None]*self.pred_layers\n",
    "\n",
    "        for i in range(self.pred_layers):\n",
    "            X, anchors[i],cls_preds[i],ofs_preds[i] = SSD_Block_Forward(X,getattr(self,f'blk_{i}'),self.sizes[i],self.ratios[i],getattr(self,f'cls_{i}'),getattr(self,f'ofs_{i}'))\n",
    "\n",
    "        anchors = torch.cat(anchors, dim=1)\n",
    "        cls_preds = concat_pred(cls_preds)\n",
    "        cls_preds = cls_preds.reshape(\n",
    "            cls_preds.shape[0],-1,self.classes +1)\n",
    "        ofs_preds = concat_pred(ofs_preds)\n",
    "        return anchors,cls_preds,ofs_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Wrappers for our training error rates\n",
    "\n",
    "class SSD_Error():\n",
    "\n",
    "    def __init__(self,path=None):\n",
    "        self.path = path\n",
    "\n",
    "        if path is not None:\n",
    "            if not os.path.exists(self.path):\n",
    "                self.hist = None\n",
    "            else:\n",
    "                try:\n",
    "                    self.hist = torch.load(self.path)\n",
    "                except:\n",
    "                    raise ValueError(f'File {path} is not of .pt format')\n",
    "\n",
    "        else:\n",
    "            self.hist = None\n",
    "\n",
    "\n",
    "        self.class_error = None\n",
    "        self.ofs_error = None\n",
    "        self.loss = None\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def Class_error(cls_preds,cls_labels):\n",
    "        return (1-float((cls_preds.argmax(dim=-1).type(\n",
    "            cls_labels.dtype) == cls_labels).sum())/cls_labels.numel())\n",
    "\n",
    "    @staticmethod\n",
    "    def Ofs_error(ofs_preds, ofs_labels, ofs_masks):\n",
    "        return float((torch.abs((ofs_labels - ofs_preds) * ofs_masks)).sum())/ofs_labels.numel()\n",
    "\n",
    "    def add(self,cls_preds,cls_labels,ofs_preds,ofs_labels,ofs_masks,loss):\n",
    "        if self.hist is None:\n",
    "\n",
    "            self.class_error = SSD_Error.Class_error(cls_preds,cls_labels)\n",
    "            self.ofs_error = SSD_Error.Ofs_error(ofs_preds,ofs_labels,ofs_masks)\n",
    "\n",
    "            if isinstance(loss,float):\n",
    "                self.loss = loss\n",
    "            elif torch.is_tensor(loss):\n",
    "                self.loss = float(loss.sum())\n",
    "            else:\n",
    "                raise ValueError(f'Loss argument is of wrong type {loss.__str__()}')\n",
    "\n",
    "\n",
    "            self.hist = torch.tensor([[self.class_error],[self.ofs_error],[self.loss]])\n",
    "\n",
    "        else:\n",
    "\n",
    "            self.class_error = SSD_Error.Class_error(cls_preds,cls_labels)\n",
    "            self.ofs_error = SSD_Error.Ofs_error(ofs_preds,ofs_labels,ofs_masks)\n",
    "\n",
    "            if isinstance(loss,float):\n",
    "                self.loss = loss\n",
    "            elif torch.is_tensor(loss):\n",
    "                self.loss = float(loss.mean())\n",
    "            else:\n",
    "                raise ValueError(f'loss argument is of wront type {loss.__str__()}')\n",
    "\n",
    "            self.hist = torch.cat((self.hist,torch.tensor([[self.class_error],[self.ofs_error],[self.loss]])),dim=1)\n",
    "\n",
    "    def save(self):\n",
    "        torch.save(self.hist,self.path)\n",
    "\n",
    "    def plot_cls_error(self):\n",
    "\n",
    "        plt.plot(self.hist[0,:])\n",
    "        plt.ylabel(\"Classification Error\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_ofs_error(self):\n",
    "        plt.plot(self.hist[1,:])\n",
    "        plt.ylabel(\"Offset Mean Absolute Error\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.hist[2,:])\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Wrapper for our Training\n",
    "\n",
    "class SSD_Training_Wrapper():\n",
    "\n",
    "    def __init__(self,net,cls_loss,ofs_loss,trainer,scheduler=None,device='cpu',save_output_path=None,save_iterations=0):\n",
    "        self.net = net\n",
    "        self.device = device\n",
    "        self.cls_loss = cls_loss\n",
    "        self.ofs_loss = ofs_loss\n",
    "        self.trainer = trainer\n",
    "        self.scheduler = scheduler\n",
    "        self.save_iteration = save_iterations\n",
    "        self.epoch = 0\n",
    "        self.prediction = None\n",
    "        self.pimg = None\n",
    "\n",
    "\n",
    "        if save_output_path is not None:\n",
    "            if isinstance(save_output_path,str):\n",
    "                if os.path.exists(save_output_path):\n",
    "                    self.save_weights_path = os.path.join(save_output_path,\"Weights.pt\")\n",
    "                    self.save_error_path = os.path.join(save_output_path,\"Error_History.pt\")\n",
    "\n",
    "                else:\n",
    "                    os.makedirs(save_output_path)\n",
    "                    self.save_weights_path = os.path.join(save_output_path,\"Weights.pt\")\n",
    "                    self.save_error_path = os.path.join(save_output_path,\"Error_History.pt\")\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f'save_error_path should be a string, is {type(save_output_path)}')\n",
    "\n",
    "        else:\n",
    "            self.save_weights_path = save_output_path\n",
    "            self.save_error_path = save_output_path\n",
    "\n",
    "        self.error = SSD_Error(path = self.save_error_path)\n",
    "\n",
    "    def calc_loss(self,cls_preds,cls_labels,ofs_preds,ofs_labels,ofs_masks):\n",
    "        batch_size, num_classes = cls_preds.shape[0], cls_preds.shape[2]\n",
    "\n",
    "        cls = self.cls_loss(cls_preds.reshape(-1, num_classes),\n",
    "                       cls_labels.reshape(-1)).reshape(batch_size, -1).mean(dim=1)\n",
    "        ofs = self.ofs_loss(ofs_preds * ofs_masks,\n",
    "                       ofs_labels * ofs_masks).mean(dim=1)\n",
    "\n",
    "        return cls + ofs\n",
    "\n",
    "    def save(self):\n",
    "        if self.save_weights_path is None or self.save_error_path is None:\n",
    "            raise ValueError(f'path for saving output has not been specified')\n",
    "        self.net.to('cpu')\n",
    "\n",
    "        torch.save(self.net.state_dict(),self.save_weights_path)\n",
    "\n",
    "        self.net.to(self.device)\n",
    "        self.error.save()\n",
    "\n",
    "    def train(self,data,num_epochs,print_cls_err=True,print_ofs_err=True,ploss = True):\n",
    "        self.net = self.net.to(self.device)\n",
    "        self.net.train()\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.trainer.zero_grad()\n",
    "            img, label = next(iter(data))\n",
    "            img, label = img.to(self.device,dtype=torch.float32), label.to(self.device,dtype=torch.float32)\n",
    "\n",
    "            anchors, cls_preds, ofs_preds = self.net(img)\n",
    "\n",
    "            bbox_labels, bbox_masks, cls_labels = multibox_target(anchors,label)\n",
    "\n",
    "            l = self.calc_loss(cls_preds,cls_labels,ofs_preds,bbox_labels,bbox_masks)\n",
    "\n",
    "            l.mean().backward()\n",
    "            self.trainer.step()\n",
    "\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "\n",
    "\n",
    "            self.error.add(cls_preds,cls_labels,ofs_preds,bbox_labels,bbox_masks,l)\n",
    "\n",
    "            if self.save_weights_path is not None and self.save_error_path is not None:\n",
    "                try:\n",
    "                    if epoch+self.epoch % self.save_iteration == 0:\n",
    "                        self.save()\n",
    "\n",
    "                except ZeroDivisionError:\n",
    "                    pass\n",
    "\n",
    "            if print_cls_err is True or print_ofs_err or ploss is True:\n",
    "                print('-------------------')\n",
    "                print(f'{self.epoch + epoch+1}th epoch')\n",
    "                print('===================')\n",
    "\n",
    "            if print_cls_err is True:\n",
    "                print(f'Class Error:{self.error.hist[0,-1]}')\n",
    "\n",
    "            if print_ofs_err is True:\n",
    "                print(f'Ofs Error:{self.error.hist[1,-1]}')\n",
    "\n",
    "            if ploss is True:\n",
    "                print(f'Loss:{self.error.hist[2,-1]}')\n",
    "\n",
    "            del img\n",
    "            del label\n",
    "\n",
    "        if self.save_weights_path is not None and self.save_error_path is not None:\n",
    "            self.save()\n",
    "\n",
    "        self.epoch = self.epoch + num_epochs\n",
    "        if self.device != 'cpu':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def predict(self,X):\n",
    "\n",
    "        if isinstance(X,list):\n",
    "            #Iterator will return a list\n",
    "            self.pimg = X\n",
    "            img = self.pimg[0].to(self.device,dtype=torch.float32)\n",
    "\n",
    "        elif isinstance(X,tuple):\n",
    "            #Dataset will return a tuple\n",
    "            X = [X[0].unsqueeze(dim=0),X[1].unsqueeze(dim=0)]\n",
    "            self.pimg = X\n",
    "            img = self.pimg[0].to(self.device,dtype=torch.float32)\n",
    "\n",
    "        else:\n",
    "            raise TypeError('Wrong type of X argument, must be either passed directly from a dataset object or as an iterator')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        anchors, cls_preds, bbox_preds = self.net(img)\n",
    "\n",
    "\n",
    "        cls_probs = F.softmax(cls_preds, dim=2).permute(0, 2, 1)\n",
    "        output = non_maximum_suppression(cls_probs, bbox_preds, anchors)\n",
    "        idx = [i for i, row in enumerate(output[0]) if row[0] != -1]\n",
    "        self.prediction = output[0,idx]\n",
    "        return self.prediction\n",
    "\n",
    "\n",
    "\n",
    "    def display(self,X = None,threshold = 0.9,predict_colour = 'green',true_colour = False,width=4):\n",
    "\n",
    "        if self.pimg is None and X is None:\n",
    "            raise ValueError('No image to display specified, please first use .predict() or pass the image directly to the display()')\n",
    "        elif self.pimg is None:\n",
    "            self.predict(X)\n",
    "\n",
    "        if self.pimg[0].size()[0] != 1:\n",
    "            raise ValueError('Cannot display multiple images, please pass one image at a time')\n",
    "\n",
    "        img,true_label  = self.pimg[0].squeeze(dim=0),self.pimg[1].squeeze(dim=0)\n",
    "        resolution = img.size()[1:]\n",
    "        trans = torchvision.transforms.ToPILImage()\n",
    "        img = trans(img)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "\n",
    "        #Remove predictions below the threshold and keep only the coordinates\n",
    "        try:\n",
    "            for i in range(len(self.prediction)):\n",
    "                if self.prediction[i][1] < threshold:\n",
    "                    if i == 0:\n",
    "                        pred_label = None\n",
    "                        print('Warning: No predictions meet the threshold')\n",
    "                        raise StopIteration\n",
    "                    pred_label = self.prediction[:i,2:]\n",
    "                    raise StopIteration\n",
    "\n",
    "        except StopIteration:\n",
    "            pass\n",
    "\n",
    "\n",
    "        #Remove negative padding if we're displaying true bounding boxes\n",
    "        if true_colour:\n",
    "            for i in range(len(true_label)):\n",
    "                if true_label[i][0] == -1:\n",
    "                    true_label = true_label[:i,1:].to(device=self.device)\n",
    "                    break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #transforming yolo to xy coordinates\n",
    "        if pred_label is not None:\n",
    "            pred_label = yolo_to_xy(pred_label)\n",
    "\n",
    "            pred_label[:,0],pred_label[:,2] = pred_label[:,0]*resolution[1],pred_label[:,2]*resolution[1]\n",
    "            pred_label[:,1],pred_label[:,3] = pred_label[:,1]*resolution[0],pred_label[:,3]*resolution[0]\n",
    "            pred_label = pred_label.tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if true_colour:\n",
    "            true_label = yolo_to_xy(true_label)\n",
    "\n",
    "            true_label[:,0],true_label[:,2] = true_label[:,0]*resolution[1],true_label[:,2]*resolution[1]\n",
    "            true_label[:,1],true_label[:,3] = true_label[:,1]*resolution[0],true_label[:,3]*resolution[0]\n",
    "            true_label = true_label.tolist()\n",
    "\n",
    "\n",
    "\n",
    "        #Drawing the bounding boxes\n",
    "        if pred_label is not None:\n",
    "            for i in pred_label:\n",
    "                draw.rectangle(i,outline=predict_colour,width=width)\n",
    "\n",
    "        if true_colour:\n",
    "            for i in true_label:\n",
    "                draw.rectangle(i,outline=true_colour,width=width)\n",
    "\n",
    "        display(img)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Evaluation of class error\n",
    "def cls_eval(cls_preds, cls_labels):\n",
    "    # Because the class prediction results are on the final dimension,\n",
    "    # `argmax` needs to specify this dimension\n",
    "    return float((cls_preds.argmax(dim=-1).type(\n",
    "        cls_labels.dtype) == cls_labels).sum())\n",
    "\n",
    "#Evaluation of offset error\n",
    "def ofs_eval(ofs_preds, ofs_labels, ofs_masks):\n",
    "    return float((torch.abs((ofs_labels - ofs_preds) * ofs_masks)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculating Loss\n",
    "def object_rec_loss(cls_preds,cls_labels,ofs_preds,ofs_labels,ofs_masks):\n",
    "    batch_size, num_classes = cls_preds.shape[0], cls_preds.shape[2]\n",
    "    cls = cls_loss(cls_preds.reshape(-1, num_classes),\n",
    "                   cls_labels.reshape(-1)).reshape(batch_size, -1).mean(dim=1)\n",
    "    ofs = ofs_loss(ofs_preds * ofs_masks,\n",
    "                     ofs_labels * ofs_masks).mean(dim=1)\n",
    "    return cls + ofs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Setting temporary parameters\n",
    "\n",
    "cls_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "ofs_loss = nn.L1Loss(reduction='none')\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "temp_sizes = [[0.2, 0.272], [0.37, 0.447], [0.54, 0.619], [0.71, 0.79],\n",
    "              [0.88, 0.961]]\n",
    "temp_ratios = [[1, 2, 0.5]] * 5\n",
    "temp_net = SSD(1,[3,16,32,64],[64,128,128,128,128],temp_sizes,temp_ratios).to('cuda:0')\n",
    "\n",
    "temp_trainer = torch.optim.SGD(temp_net.parameters(), lr=0.2, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_net.train()\n",
    "\n",
    "training_1_dir = '../Cyclist_Detection/Training_1/Error_hist.pt'\n",
    "training_2_dir = '../Cyclist_Detection/Training_1/Error_History.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SSD_Test_Training = SSD_Training_Wrapper(temp_net,cls_loss,ofs_loss,temp_trainer,device='cuda:0',save_output_path='../Cyclist_Detection/Training_1/',save_iterations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SSD_Test_Training.train(train_iter,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_predict = SSD_Test_Training.predict(train[402])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SSD_Test_Training.display(threshold=0.7,true_colour='yellow')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
