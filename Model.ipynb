{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.utils\n",
    "import torchvision.transforms\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageDraw, Image\n",
    "%matplotlib inline\n",
    "\n",
    "torch.set_printoptions(threshold=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_dir = \"../Cyclist_Detection/Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing our Dataset class\n",
    "class YoloPtDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset with tensor format images and Yolo format labels\"\"\"\n",
    "\n",
    "    def __init__(self,root_dir,data_type=None,transform=None):\n",
    "        if data_type is None:\n",
    "            try:\n",
    "                self.paths = pd.read_csv(os.path.join(root_dir,\"src\",\"src.csv\"))\n",
    "                self.paths.values.astype(str)\n",
    "            except:\n",
    "                raise TypeError(\"The directory doesn't have a src file with the paths to all the images\")\n",
    "        elif data_type == 'train' or data_type == 'Train':\n",
    "            try:\n",
    "                self.paths = pd.read_csv(os.path.join(root_dir,\"src\",\"train_src.csv\"))\n",
    "                self.paths.values.astype(str)\n",
    "            except:\n",
    "                raise TypeError(\"The directory doesn't have a train_src file with the paths to all the images\")\n",
    "\n",
    "        elif data_type == 'val' or data_type == 'Validate' or data_type == 'validate':\n",
    "            try:\n",
    "                self.paths = pd.read_csv(os.path.join(root_dir,\"src\",\"val_src.csv\"))\n",
    "                self.paths.values.astype(str)\n",
    "            except:\n",
    "                raise TypeError(\"The directory doesn't have a val_src file with the paths to all the images\")\n",
    "\n",
    "        elif data_type == 'test' or data_type == 'Test':\n",
    "            try:\n",
    "                self.paths = pd.read_csv(os.path.join(root_dir,\"src\",\"test_src.csv\"))\n",
    "                self.paths.values.astype(str)\n",
    "            except:\n",
    "                raise TypeError(\"The directory doesn't have a test_src file with the paths to all the images\")\n",
    "        else:\n",
    "            raise ValueError(\"Wrong argument has been passed through data_type, must be one of: train,test,val\")\n",
    "\n",
    "\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path, label_path = self.paths.iloc[idx,1:].values\n",
    "\n",
    "        return torch.load(img_path),torch.load(label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Modyifying yolo formatted coordinates to (x_left,y_left,x_right,y_right)\n",
    "def yolo_to_xy(yolo):\n",
    "    _yolo = yolo.clone()\n",
    "    w,h = yolo[:,2]/2,yolo[:,3]/2\n",
    "    _yolo[:,0] = yolo[:,0]-w\n",
    "    _yolo[:,1] = yolo[:,1]-h\n",
    "    _yolo[:,2] = yolo[:,0]+w\n",
    "    _yolo[:,3] = yolo[:,1]+h\n",
    "    return _yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def xy_to_yolo(xy):\n",
    "    if xy.shape[1] == 5:\n",
    "        x1,y1,x2,y2 = xy[:,1],xy[:,2],xy[:,3],xy[:,4]\n",
    "        cx = (x1 +x2)/2\n",
    "        cy = (y1 + y2)/2\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1\n",
    "\n",
    "        return torch.stack((xy[:,0],cx,cy,w,h),axis=-1)\n",
    "\n",
    "    elif xy.shape[1] == 4:\n",
    "        x1,y1,x2,y2 = xy[:,0],xy[:,1],xy[:,2],xy[:,3]\n",
    "        cx = (x1 +x2)/2\n",
    "        cy = (y1 + y2)/2\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1\n",
    "\n",
    "        return torch.stack((cx,cy,w,h),axis=-1)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f'Wrong input shape:{xy.shape},takes [i,4] or [i,5]')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(xy_to_yolo(yolo_to_xy(torch.tensor([[0.5,0.5,1,1]]))))\n",
    "print(xy_to_yolo(yolo_to_xy(torch.tensor([[0.5,0.5,1,1],[0.5,0.5,1,1]]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Displaying a single image with bounding boxes\n",
    "def Display_yolo(data,colour='green',width = 5,dtype='yolo'):\n",
    "\n",
    "    img, label = data[0],data[1]\n",
    "    resolution = img.size()[1:3]\n",
    "    trans = torchvision.transforms.ToPILImage()\n",
    "    img = trans(img)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    #Removing negative padding\n",
    "    for i in range(len(label)):\n",
    "        if label[i][0] == -1:\n",
    "            label = label[0:i]\n",
    "            break\n",
    "\n",
    "    #Transforming to bounding box xy coordinates\n",
    "    if dtype == 'yolo':\n",
    "        label = yolo_to_xy(label[:,1:5])\n",
    "    else:\n",
    "        label = label[:,1:5]\n",
    "    label[:,0],label[:,2] = label[:,0]*resolution[1],label[:,2]*resolution[1]\n",
    "    label[:,1],label[:,3] = label[:,1]*resolution[0],label[:,3]*resolution[0]\n",
    "    label = label.tolist()\n",
    "\n",
    "    #Re-calculates the yolo format to top-left and bottom right points\n",
    "    for i in label:\n",
    "        draw.rectangle(i,outline=colour,width=4)\n",
    "\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train = YoloPtDataset(Dataset_dir,data_type='train')\n",
    "val = YoloPtDataset(Dataset_dir,data_type='val')\n",
    "test = YoloPtDataset(Dataset_dir,data_type='test')\n",
    "train_iter = torch.utils.data.DataLoader(train,batch_size=3,shuffle=False)\n",
    "val_iter = torch.utils.data.DataLoader(val,batch_size=len(val),shuffle=False)\n",
    "test_iter = torch.utils.data.DataLoader(test,batch_size=len(test),shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Display_yolo(train[402],colour='yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def Intersection_Over_Union(anchors,bounding_boxes,ltype='yolo'):\n",
    "    if ltype == 'yolo':\n",
    "        if bounding_boxes.shape[1] == 5:\n",
    "            b1 = yolo_to_xy(anchors)\n",
    "            b2 = yolo_to_xy(bounding_boxes[:,1:5])\n",
    "            bounding_areas = bounding_boxes[:,3]*bounding_boxes[:,4]\n",
    "        elif bounding_boxes.shape[1] == 4:\n",
    "            b1 = yolo_to_xy(anchors)\n",
    "            b2 = yolo_to_xy(bounding_boxes)\n",
    "            bounding_areas = bounding_boxes[:,2]*bounding_boxes[:,3]\n",
    "        else:\n",
    "            raise ValueError(f'Wrong shape of input {bounding_boxes.shape}, takes input [i,5] or [i,4]')\n",
    "\n",
    "    else:\n",
    "        b1 = anchors.clone()\n",
    "        b2 = bounding_boxes[:,1:5]\n",
    "\n",
    "\n",
    "    inter_upperlefts = torch.max(b1[:, None, :2], b2[:, :2])\n",
    "    inter_lowerrights = torch.min(b1[:, None, 2:], b2[:, 2:])\n",
    "\n",
    "\n",
    "\n",
    "    inters = (inter_lowerrights-inter_upperlefts).clamp(min=0)\n",
    "    intersection = inters[:, :, 0] * inters[:, :, 1]\n",
    "\n",
    "    anchor_areas = anchors[:,2]*anchors[:,3]\n",
    "\n",
    "\n",
    "    inter_areas = inters[:, :, 0] * inters[:, :, 1]\n",
    "    union_areas = anchor_areas[:, None] + bounding_areas - inter_areas\n",
    "\n",
    "    return inter_areas/union_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "temp_1 = torch.tensor([[0.25,0.25,0.5,0.5],[0.5,0.5,1,1]])\n",
    "temp_2 = torch.tensor([[0,0.5,0.5,0.5,0.5],[0,0.5,0.5,1,1]])\n",
    "print(Intersection_Over_Union(temp_1,temp_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def assign_anchor_to_bbox(anchors,ground_truth,device='cpu',iou_treshold = 0.7):\n",
    "    \"\"\"Assign closest ground-truth bounding boxes to anchor boxes.\"\"\"\n",
    "    num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]\n",
    "    # Element x_ij in the i-th row and j-th column is the IoU of the anchor\n",
    "    # box i and the ground-truth bounding box j\n",
    "    jaccard = Intersection_Over_Union(anchors, ground_truth)\n",
    "    # Initialize the tensor to hold the assigned ground-truth bounding box for\n",
    "    # each anchor\n",
    "    anchors_bbox_map = torch.full((num_anchors,), -1, dtype=torch.long,\n",
    "                              device=device)\n",
    "    # Assign ground-truth bounding boxes according to the threshold\n",
    "    max_ious, indices = torch.max(jaccard, dim=1)\n",
    "    anc_i = torch.nonzero(max_ious >= 0.5).reshape(-1)\n",
    "    box_j = indices[max_ious >= 0.5]\n",
    "    anchors_bbox_map[anc_i] = box_j\n",
    "    col_discard = torch.full((num_anchors,), -1)\n",
    "    row_discard = torch.full((num_gt_boxes,), -1)\n",
    "    for _ in range(num_gt_boxes):\n",
    "        max_idx = torch.argmax(jaccard)  # Find the largest IoU\n",
    "        box_idx = (max_idx % num_gt_boxes).long()\n",
    "        anc_idx = (max_idx / num_gt_boxes).long()\n",
    "        anchors_bbox_map[anc_idx] = box_idx\n",
    "        jaccard[:, box_idx] = col_discard\n",
    "        jaccard[anc_idx, :] = row_discard\n",
    "    return anchors_bbox_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "temp_1 = torch.tensor([[0.25,0.25,0.5,0.5],[0.5,0.5,1,1]])\n",
    "temp_2 = torch.tensor([[0.5,0.5,0.5,0.5],[0.5,0.5,1,1]])\n",
    "print(assign_anchor_to_bbox(temp_1,temp_2,iou_treshold=0.20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calc_offsets(anchors,bounding_boxes,eps=1e-6):\n",
    "    offset_xy = 10 * (bounding_boxes[:, :2] - anchors[:, :2]) / anchors[:, 2:]\n",
    "    offset_wh = 5 * torch.log(eps + bounding_boxes[:, 2:] / anchors[:, 2:])\n",
    "    return torch.cat([offset_xy, offset_wh], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(calc_offsets(temp_1,temp_2[:,1:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def multibox_target(anchors, labels,iou_threshold = 0.5):\n",
    "    \"\"\"Label anchor boxes using ground-truth bounding boxes.\"\"\"\n",
    "    batch_size, anchors = labels.shape[0], anchors.squeeze(0)\n",
    "    batch_offset, batch_mask, batch_class_labels = [], [], []\n",
    "    device, num_anchors = anchors.device, anchors.shape[0]\n",
    "    for i in range(batch_size):\n",
    "        label = labels[i, :, :]\n",
    "        anchors_bbox_map = assign_anchor_to_bbox(\n",
    "            anchors,label[:,1:], device,iou_treshold=iou_threshold)\n",
    "        bbox_mask = ((anchors_bbox_map >= 0).float().unsqueeze(-1)).repeat(\n",
    "            1, 4)\n",
    "        # Initialize class labels and assigned bounding box coordinates with\n",
    "        # zeros\n",
    "        class_labels = torch.zeros(num_anchors, dtype=torch.long,\n",
    "                                   device=device)\n",
    "        assigned_bb = torch.zeros((num_anchors, 4), dtype=torch.float32,\n",
    "                                  device=device)\n",
    "        # Label classes of anchor boxes using their assigned ground-truth\n",
    "        # bounding boxes. If an anchor box is not assigned any, we label its\n",
    "        # class as background (the value remains zero)\n",
    "        indices_true = torch.nonzero(anchors_bbox_map >= 0)\n",
    "        bb_idx = anchors_bbox_map[indices_true]\n",
    "        class_labels[indices_true] = label[bb_idx, 0].long() + 1\n",
    "        assigned_bb[indices_true] = label[bb_idx, 1:]\n",
    "        # Offset transformation\n",
    "        print(f'iter {i}:')\n",
    "        print('---------')\n",
    "        print(f'anchors:{anchors}')\n",
    "        print(f'assigned_bb:{assigned_bb}')\n",
    "        offset = calc_offsets(anchors, assigned_bb) * bbox_mask\n",
    "        print(f'offset:{offset}')\n",
    "        batch_offset.append(offset.reshape(-1))\n",
    "        batch_mask.append(bbox_mask.reshape(-1))\n",
    "        batch_class_labels.append(class_labels)\n",
    "    bbox_offset = torch.stack(batch_offset)\n",
    "    bbox_mask = torch.stack(batch_mask)\n",
    "    class_labels = torch.stack(batch_class_labels)\n",
    "    return (bbox_offset, bbox_mask, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "multibox_target(temp_1,next(iter(train_iter))[1],iou_threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Examples from d2l:\n",
    "ground_truth_ = torch.tensor([[0, 0.1, 0.08, 0.52, 0.92],\n",
    "                             [1, 0.55, 0.2, 0.9, 0.88]])\n",
    "anchors_ = torch.tensor([[0, 0.1, 0.2, 0.3], [0.15, 0.2, 0.4, 0.4],\n",
    "                        [0.63, 0.05, 0.88, 0.98], [0.66, 0.45, 0.8, 0.8],\n",
    "                        [0.57, 0.3, 0.92, 0.9]])\n",
    "ground_truth_ = xy_to_yolo(ground_truth_)\n",
    "anchors_ = xy_to_yolo(anchors_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "multibox_target(anchors_.unsqueeze(dim=0),ground_truth_.unsqueeze(dim=0),iou_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0:\n",
      "---------\n",
      "anchors:tensor([[0.1000, 0.2000, 0.2000, 0.2000],\n",
      "        [0.2750, 0.3000, 0.2500, 0.2000],\n",
      "        [0.7550, 0.5150, 0.2500, 0.9300],\n",
      "        [0.7300, 0.6250, 0.1400, 0.3500],\n",
      "        [0.7450, 0.6000, 0.3500, 0.6000]])\n",
      "assigned_bb:tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3100, 0.5000, 0.4200, 0.8400],\n",
      "        [0.7250, 0.5400, 0.3500, 0.6800],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7250, 0.5400, 0.3500, 0.6800]])\n",
      "offset:tensor([[-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "        [ 1.4000e+00,  1.0000e+01,  2.5940e+00,  7.1754e+00],\n",
      "        [-1.2000e+00,  2.6882e-01,  1.6824e+00, -1.5655e+00],\n",
      "        [-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
      "        [-5.7143e-01, -1.0000e+00,  4.1723e-06,  6.2582e-01]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0000e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,  1.4000e+00,\n",
       "           1.0000e+01,  2.5940e+00,  7.1754e+00, -1.2000e+00,  2.6882e-01,\n",
       "           1.6824e+00, -1.5655e+00, -0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
       "          -0.0000e+00, -5.7143e-01, -1.0000e+00,  4.1723e-06,  6.2582e-01]]),\n",
       " tensor([[0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
       "          1., 1.]]),\n",
       " tensor([[0, 1, 2, 0, 2]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multibox_target(anchors_.unsqueeze(dim=0),ground_truth_.unsqueeze(dim=0),iou_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Code from d2l, to re-write for yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generating priors\n",
    "def multibox_prior(data, sizes, ratios):\n",
    "    \"\"\"Generate anchor boxes with different shapes centered on each pixel.\"\"\"\n",
    "    in_height, in_width = data.shape[-2:]\n",
    "    device, num_sizes, num_ratios = data.device, len(sizes), len(ratios)\n",
    "    boxes_per_pixel = (num_sizes + num_ratios - 1)\n",
    "    size_tensor = torch.tensor(sizes, device=device)\n",
    "    ratio_tensor = torch.tensor(ratios, device=device)\n",
    "    # Offsets are required to move the anchor to the center of a pixel. Since\n",
    "    # a pixel has height=1 and width=1, we choose to offset our centers by 0.5\n",
    "    offset_h, offset_w = 0.5, 0.5\n",
    "    steps_h = 1.0 / in_height  # Scaled steps in y axis\n",
    "    steps_w = 1.0 / in_width  # Scaled steps in x axis\n",
    "    # Generate all center points for the anchor boxes\n",
    "    center_h = (torch.arange(in_height, device=device) + offset_h) * steps_h\n",
    "    center_w = (torch.arange(in_width, device=device) + offset_w) * steps_w\n",
    "    shift_y, shift_x = torch.meshgrid(center_h, center_w)\n",
    "    shift_y, shift_x = shift_y.reshape(-1), shift_x.reshape(-1)\n",
    "    # Generate `boxes_per_pixel` number of heights and widths that are later\n",
    "    # used to create anchor box corner coordinates (xmin, xmax, ymin, ymax)\n",
    "    w = torch.cat((size_tensor * torch.sqrt(ratio_tensor[0]),\n",
    "                   sizes[0] * torch.sqrt(ratio_tensor[1:])))\\\n",
    "                   * in_height / in_width  # Handle rectangular inputs\n",
    "    h = torch.cat((size_tensor / torch.sqrt(ratio_tensor[0]),\n",
    "                   sizes[0] / torch.sqrt(ratio_tensor[1:])))\n",
    "    # Divide by 2 to get half height and half width\n",
    "    anchor_manipulations = torch.stack((-w, -h, w, h)).T.repeat(\n",
    "        in_height * in_width, 1) / 2\n",
    "    # Each center point will have `boxes_per_pixel` number of anchor boxes, so\n",
    "    # generate a grid of all anchor box centers with `boxes_per_pixel` repeats\n",
    "    out_grid = torch.stack([shift_x, shift_y, shift_x, shift_y],\n",
    "                           dim=1).repeat_interleave(boxes_per_pixel, dim=0)\n",
    "    output = out_grid + anchor_manipulations\n",
    "    return output.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Functions for SSD\n",
    "def cls_predictor(num_inputs,priors,classes):\n",
    "    return nn.Conv2d(num_inputs,priors*(classes+1),kernel_size=3,padding=1)\n",
    "\n",
    "def offset_predictor(num_inputs,priors):\n",
    "    return nn.Conv2d(num_inputs,priors*4,kernel_size=3,padding=1)\n",
    "\n",
    "def flatten_pred(pred):\n",
    "    return torch.flatten(pred.permute(0,2,3,1),start_dim=1)\n",
    "\n",
    "def concat_pred(pred):\n",
    "    return torch.cat([flatten_pred(p) for p in pred], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Building blocks of SSD\n",
    "def SSD_Conv_Block(num_inputs,num_outputs,convolutions=2,downsample=2):\n",
    "    blk = []\n",
    "    for _ in range(convolutions):\n",
    "        blk.append(nn.Conv2d(num_inputs,num_outputs,kernel_size=3,padding=1))\n",
    "        blk.append(nn.BatchNorm2d(num_outputs))\n",
    "        blk.append(nn.ReLU())\n",
    "        num_inputs = num_outputs\n",
    "    blk.append(nn.MaxPool2d(downsample))\n",
    "    return nn.Sequential(*blk)\n",
    "\n",
    "\n",
    "def SSD_Base_Block(channels):\n",
    "    if not isinstance(channels,list):\n",
    "        raise TypeError(\"channels argument must be list of channels\")\n",
    "\n",
    "    blk = []\n",
    "    for i in range(len(channels)-1):\n",
    "        blk.append(SSD_Conv_Block(channels[i],channels[i+1]))\n",
    "    return nn.Sequential(*blk)\n",
    "\n",
    "def SSD_Block_Forward(data,blk,sizes,ratios,cls_predictor,offset_predictor):\n",
    "    Y = blk(data)\n",
    "    priors = multibox_prior(Y,sizes=sizes,ratios=ratios)\n",
    "    cls_preds = cls_predictor(Y)\n",
    "    offset_preds = offset_predictor(Y)\n",
    "    return Y,priors, cls_preds, offset_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SSD(nn.Module):\n",
    "    def __init__(self,classes,base_channels,remaining_channels,sizes,ratios,**kwargs):\n",
    "        super(SSD,self).__init__(**kwargs)\n",
    "\n",
    "        if not isinstance(base_channels,list):\n",
    "            raise TypeError(\"base_channels argument must be list of channels\")\n",
    "\n",
    "        if not isinstance(remaining_channels,list):\n",
    "            raise TypeError(\"remaining_channels argument must be list of channels\")\n",
    "\n",
    "        self.remaining_channels = remaining_channels\n",
    "        self.sizes = sizes\n",
    "        self.ratios = ratios\n",
    "        self.priors = len(self.sizes[0]) + len(self.ratios[0]) - 1\n",
    "\n",
    "        if not len(self.remaining_channels) == self.priors+1:\n",
    "            raise TypeError(f'The amount of priors {self.priors+1}, does not match amount of layers {len(self.remaining_channels)}')\n",
    "\n",
    "\n",
    "        self.classes = classes\n",
    "        self.base_channels = base_channels\n",
    "\n",
    "\n",
    "\n",
    "        self.blk_0 = SSD_Base_Block(self.base_channels)\n",
    "        self.cls_0 = cls_predictor(self.base_channels[-1],self.priors,self.classes)\n",
    "        self.ofs_0 = offset_predictor(self.base_channels[-1],self.priors)\n",
    "\n",
    "\n",
    "        for i in range(len(self.remaining_channels)):\n",
    "\n",
    "            if i == len(self.remaining_channels)-2:\n",
    "\n",
    "                setattr(self,f'blk_{i+1}',nn.AdaptiveMaxPool2d((1,1)))\n",
    "                setattr(self,f'cls_{i+1}',cls_predictor(self.remaining_channels[i+1],self.priors,self.classes))\n",
    "                setattr(self,f'ofs_{i+1}',offset_predictor(self.remaining_channels[i+1],self.priors))\n",
    "                self.pred_layers = i+2\n",
    "\n",
    "                break\n",
    "\n",
    "\n",
    "            setattr(self,f'blk_{i+1}',SSD_Conv_Block(self.remaining_channels[i],self.remaining_channels[i+1]))\n",
    "            setattr(self,f'cls_{i+1}',cls_predictor(self.remaining_channels[i+1],self.priors,self.classes))\n",
    "            setattr(self,f'ofs_{i+1}',offset_predictor(self.remaining_channels[i+1],self.priors))\n",
    "\n",
    "    def forward(self,X):\n",
    "        anchors, cls_preds,ofs_preds = [None]*self.pred_layers, [None]*self.pred_layers,[None]*self.pred_layers\n",
    "\n",
    "        for i in range(self.pred_layers):\n",
    "            X, anchors[i],cls_preds[i],ofs_preds[i] = SSD_Block_Forward(X,getattr(self,f'blk_{i}'),self.sizes[i],self.ratios[i],getattr(self,f'cls_{i}'),getattr(self,f'ofs_{i}'))\n",
    "\n",
    "        anchors = torch.cat(anchors, dim=1)\n",
    "        cls_preds = concat_pred(cls_preds)\n",
    "        cls_preds = cls_preds.reshape(\n",
    "            cls_preds.shape[0],-1,self.classes +1)\n",
    "        ofs_preds = concat_pred(ofs_preds)\n",
    "        return anchors,cls_preds,ofs_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict([('blk_0',\n",
       "               Sequential(\n",
       "                 (0): Sequential(\n",
       "                   (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                   (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                   (2): ReLU()\n",
       "                   (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                   (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                   (5): ReLU()\n",
       "                   (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "                 )\n",
       "                 (1): Sequential(\n",
       "                   (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                   (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                   (2): ReLU()\n",
       "                   (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                   (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                   (5): ReLU()\n",
       "                   (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "                 )\n",
       "                 (2): Sequential(\n",
       "                   (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                   (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                   (2): ReLU()\n",
       "                   (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                   (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                   (5): ReLU()\n",
       "                   (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "                 )\n",
       "               )),\n",
       "              ('cls_0',\n",
       "               Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       "              ('ofs_0',\n",
       "               Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       "              ('blk_1',\n",
       "               Sequential(\n",
       "                 (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                 (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                 (2): ReLU()\n",
       "                 (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                 (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                 (5): ReLU()\n",
       "                 (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "               )),\n",
       "              ('cls_1',\n",
       "               Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       "              ('ofs_1',\n",
       "               Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       "              ('blk_2',\n",
       "               Sequential(\n",
       "                 (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                 (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                 (2): ReLU()\n",
       "                 (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                 (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                 (5): ReLU()\n",
       "                 (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "               )),\n",
       "              ('cls_2',\n",
       "               Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       "              ('ofs_2',\n",
       "               Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       "              ('blk_3',\n",
       "               Sequential(\n",
       "                 (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                 (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                 (2): ReLU()\n",
       "                 (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                 (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                 (5): ReLU()\n",
       "                 (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "               )),\n",
       "              ('cls_3',\n",
       "               Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       "              ('ofs_3',\n",
       "               Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       "              ('blk_4', AdaptiveMaxPool2d(output_size=(1, 1))),\n",
       "              ('cls_4',\n",
       "               Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))),\n",
       "              ('ofs_4',\n",
       "               Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)))]),\n",
       " 'remaining_channels': [64, 128, 128, 128, 128],\n",
       " 'sizes': [[0.2, 0.272],\n",
       "  [0.37, 0.447],\n",
       "  [0.54, 0.619],\n",
       "  [0.71, 0.79],\n",
       "  [0.88, 0.961]],\n",
       " 'ratios': [[1, 2, 0.5], [1, 2, 0.5], [1, 2, 0.5], [1, 2, 0.5], [1, 2, 0.5]],\n",
       " 'priors': 4,\n",
       " 'classes': 1,\n",
       " 'base_channels': [3, 16, 32, 64],\n",
       " 'pred_layers': 5}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Temp\n",
    "temp_sizes = [[0.2, 0.272], [0.37, 0.447], [0.54, 0.619], [0.71, 0.79],\n",
    "         [0.88, 0.961]]\n",
    "temp_ratios = [[1, 2, 0.5]] * 5\n",
    "temp_net = SSD(1,[3,16,32,64],[64,128,128,128,128],temp_sizes,temp_ratios)\n",
    "\n",
    "temp_net.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\48510\\anaconda3\\envs\\torchenv\\lib\\site-packages\\torch\\functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "temp_img, temp_label = next(iter(train_iter))\n",
    "temp_img, temp_label = temp_img.type(torch.float32),temp_label.type(torch.float32)\n",
    "output_anchors, output_cls_preds, output_ofs_preds = temp_net(temp_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 174084, 4])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_anchors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 174084, 2])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_cls_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 696336])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ofs_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.3301,  0.4487,  0.0449,  0.1240],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.3772,  0.4443,  0.0444,  0.1191],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.4102,  0.4414,  0.0342,  0.1133],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000,  0.0000,  0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torchenv]",
   "language": "python",
   "name": "conda-env-torchenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
